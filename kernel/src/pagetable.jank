
//pagetables allow for mapping virtual addresses to physical addresses
//if long mode is enabled, then all memory accesses are treated as virtual addresses and go through
//the pagetable to be translated into a physical address which then gets accessed

//the translation process happens like this:
// the cpu gets a 64-bit virtual address:
// XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX
//   60   56   52   48   44   40   36   32   28   24   20   16   12    8    4    0
// It splits the address into 6 chunks (big endian):
// XXXXXXXXXXXXXXXX XXXXXXXXX XXXXXXXXX XXXXXXXXX XXXXXXXXX XXXXXXXXXXXX
// |------16------| |---9---| |---9---| |---9---| |---9---| |----12----|
// first, it ensures the address is 'canonical', the most significant 16 bits must be a valid sign extension
//  of the 47th bit (0-indexed)
// then it uses the first 9-bit chunk to index into the top level pagetable stored in the CR3 register. 
// each pagetable takes up one page (4096 bytes) and each table can store up to 512 entries (512 = 2^9)
// the pagetable entry will contain information pointing to the physical address of the next pagetable
//  page, and the process repeats until we get to the last 9-bit chunk. 
// this last entry will have the physical address of the actual page we want to access. Then, the last
//  12-bit chunk can be used to index into the page to find the exact byte we want (4096 = 2^12)

//some caveats to this process:
// - if in the middle somewhere, the PTE_HUGEPG flag is turned on, the pagetable walk terminates early
// - if you don't have the correct permissions (currently on a user data segment and PTE_USER is off), 
//    a page fault happens

//about the Page Attribute Table (PAT)
// it used to be that you could only define cache-related behaviour using the PWT and PCD bits. 
// with the introduction of the PAT bit, PAT, PCD, PWT bits instead become interpreted as a 3-bit index into the
// IA32_PAT_MSR register, each entry describing some different caching behaviour. To maintain legacy behaviour,
// I setup the PAT MSR so that with PAT = 0, the behaviour corresponds to non-PAT usage.

typedef u64 pte_t;              
typedef pte_t* pagetable_t;

[__GLOBAL_FIRST__] u64 PTE_PRESENT      = $u64 1 << $u64 0;
[__GLOBAL_FIRST__] u64 PTE_WRITEABLE    = $u64 1 << $u64 1;   //can only write if every entry along the walk has this flag 
[__GLOBAL_FIRST__] u64 PTE_USER         = $u64 1 << $u64 2;   //user can only access if every entry along the walk has this flag 
[__GLOBAL_FIRST__] u64 PTE_HUGEPG       = $u64 1 << $u64 7;
[__GLOBAL_FIRST__] u64 PTE_NX           = $u64 1 << $u64 63;  //*NOT* executable flag, can only execute if no entry along the walk has this flag
[__GLOBAL_FIRST__] u64 PTE_PWT          = $u64 1 << $u64 3;   // Write Through
[__GLOBAL_FIRST__] u64 PTE_PCD          = $u64 1 << $u64 4;   // Cache Disable
[__GLOBAL_FIRST__] u64 PTE_PAT          = $u64 1 << $u64 7;   // Page Attribute Table

[__GLOBAL_FIRST__] u32 IA32_PAT_MSR     = $u32 0x277;
[__GLOBAL_FIRST__] u64 PAT_UC           = $u64 0;   // Uncacheable : all accesses are uncacheable, write combining is not allowed, speculative reads are not allowed
[__GLOBAL_FIRST__] u64 PAT_WC           = $u64 1;   // Write Combining : all accesses are uncacheable, write combining is allowed, speculative reads are allowed
[__GLOBAL_FIRST__] u64 PAT_WT           = $u64 4;   // Write Through
[__GLOBAL_FIRST__] u64 PAT_WP           = $u64 5;   // Write Protect
[__GLOBAL_FIRST__] u64 PAT_WB           = $u64 6;   // Write Back 
[__GLOBAL_FIRST__] u64 PAT_UCM          = $u64 7;   // Uncached : same as UC, but can be overridden by a WC MTRR

void enable_nx() {
    //check if nx is supported
    u64 x;
    asm!("mov $0x80000001, %eax");
    asm!("cpuid");
    asm!("mov %edx, {x}");
    assert(x & $u64 (1 << 20), "enable_nx() : NX should be supported");

    //enable NX bit in EFER
    wrmsr(EFER_MSR, rdmsr(EFER_MSR) | $u64 (1 << 11));
}

i32 init_paging() {
    enable_nx();

    //setup PAT
    {
        u64 pat = 0x0;
        pat |= PAT_WB << $u64 0;
        pat |= PAT_WT << $u64 8;
        pat |= PAT_UCM << $u64 16;
        pat |= PAT_UC << $u64 24;
        pat |= PAT_WC << $u64 32;
        pat |= PAT_WT << $u64 40;
        pat |= PAT_UCM << $u64 48;
        pat |= PAT_UC << $u64 56;
        wrmsr(IA32_PAT_MSR, pat);
    }

    return 0;
}

//given a PAT caching type, gives you the pte permission bits to get that type
u64 pte_pat_bits(u64 type) {
    if(type == PAT_WB)          return 0x0;
    else if(type == PAT_WT)     return PTE_PWT;
    else if(type == PAT_UCM)    return PTE_PCD;
    else if(type == PAT_UC)     return PTE_PCD | PTE_PWT;
    else if(type == PAT_WC)     return PTE_PAT;
    else panic("pte_pat_bits() : unexpected caching type");
    return 0x0;
}

void* pte_get_addr(pte_t pte) {
    return $void* (pte & 0x7fffffffff000);
}

u64 pte_get_permission_flags(pte_t pte) {
    u64 flag_mask = PTE_PRESENT | PTE_USER | PTE_WRITEABLE | PTE_NX;
    return pte & flag_mask;
}

pte_t pte_create_new(u64 flags) {
    pagetable_t pt = pt_alloc_new();
    return ($u64 pt) | flags | PTE_PRESENT;
}

pte_t pte_create_new(void* paddr, u64 flags) {
    return ($u64 paddr) | flags | PTE_PRESENT;
}

u64 vaddr_get_pt_ind(void* vaddr, i32 level) {
    assert(3 >= level && level >= 0, "vaddr_get_pt_ind() : invalid level");
    return (($u64 vaddr) & (0x1ff << $u64 (12 + level * 9))) >> $u64 (12 + level * 9);
}

pagetable_t pt_alloc_new() {
    pagetable_t pt = $pagetable_t palloc();
    memset($void* pt, 0, PAGE_SIZE);
    return pt;
}

pagetable_t pt_get_current() {
    pagetable_t ret;
    asm!("mov %cr3, %rax");
    asm!("mov %rax, {ret}");
    return ret;
}

void pt_switch(pagetable_t npt) {
    asm!("movq {npt}, %rax");
    asm!("mov %rax, %cr3");
}

// kernel pagetable is stored in GSData struct so we can swap back to it at anytime
void pt_switch_to_kernel() {
    asm!("movq %gs:0, %rax");
    asm!("mov %rax, %cr3");
}

//given virtual page address, walks along the provided pagetable
//stops traversing at level 'level' and returns the PTE to the next page at that level
// if(create), then it creates stuff wherever needed
// else, it returns the PTE where it stopped (so PTE_PRESENT should be unset)
//updates level to indicate the actual level reached
pte_t pt_walk(pagetable_t pt, void* vaddr, i32& level, i32 create, u64 flags, u64 leaf_flags) {
    assert($u64 vaddr % PAGE_SIZE == $u64 0, "pt_walk() : vaddr must be page aligned");
    assert(3 >= level && level >= 0, "pt_walk() : invalid level");
    //print("PT WALK VADDR : ");
    //println(vaddr);
    i32 _level = level;
    level = 3;
    for(; level >= _level; level--){
        u64 ind = vaddr_get_pt_ind(vaddr, level);
        pte_t pte = pt[ind];

        //check if pte is not present
        if(!(pte & PTE_PRESENT)) {
            //if create flag is not set, return non-present pte
            if(!create) return pte;

            //create new page and map it
            if(level != 0) pt[ind] = pte_create_new(flags);
            else pt[ind] = pte_create_new(leaf_flags);
            pte = pt[ind];
        }

        //if create flag is on and leaf_flags includes PTE_USER, add PTE_USER to flags
        if($i32 (leaf_flags & PTE_USER) && create) {
            pt[ind] |= PTE_USER;
        }

        //if create flag is on and leaf_flags includes PTE_WRITEABLE, add PTE_WRITEABLE to flags
        if($i32 (leaf_flags & PTE_WRITEABLE) && create) {
            pt[ind] |= PTE_WRITEABLE;
        }

        //check if this is a huge page
        if(level == 2 && $i32 (pte & PTE_HUGEPG)) {
            return pte;
        }
        if(level == 1 && $i32 (pte & PTE_HUGEPG)) {
            return pte;
        }

        //check if we should stop
        if(level == _level) {
            return pte;
        }

        //traverse down
        pt = $pagetable_t pte_get_addr(pte);
    }

    panic("pt_walk() : should not exit for loop");
    return $pte_t 0;
}

//nocreate pt_walk
//panics if vaddr is not mapped to the specified level
pte_t pt_walk(pagetable_t pt, void* vaddr, i32& level) {
    pte_t pte = pt_walk(pt, vaddr, level, 0, $u64 0, $u64 0);
    assert(pte & PTE_PRESENT, "pt_walk() : vaddr mapping not found");
    return pte;
}

pte_t pt_get_leaf_pte(pagetable_t pt, void* vaddr) {
    i32 level = 0;
    return pt_walk(pt, $void* (($u64 vaddr) & ~0xFFF), level);
}

i32 pt_is_vaddr_mapped(pagetable_t pt, void* vaddr) {
    i32 level = 0;
    pte_t pte = pt_walk(pt, vaddr, level, 0, $u64 0, $u64 0);
    return $i32 (pte & PTE_PRESENT);
}

//creates a pagetable mapping from vaddr -> paddr
//panics if vaddr is already mapped
void pt_map_page(pagetable_t pt, void* vaddr, void* paddr, u64 leaf_flags) {
    //ensure that vaddr, paddr are page aligned
    assert(($u64 vaddr) % PAGE_SIZE == 0x0, "pt_map_page() : vaddr must be page aligned");
    assert(($u64 paddr) % PAGE_SIZE == 0x0, "pt_map_page() : paddr must be page aligned");

    //check if this page is already mapped
    assert(!pt_is_vaddr_mapped(pt, vaddr), "pt_map_page() : vaddr already mapped");

    //traverse pt to level 1
    i32 level = 1;
    pte_t pte = pt_walk(pt, vaddr, level, 1, PTE_WRITEABLE, leaf_flags);
    assert(level == 1, "pt_map_page() : we should get level 1 pte");
    assert(!(pte & PTE_HUGEPG), "pt_map_page() : pte should not correspond to huge page");

    //traverse down to level 0 pt
    pt = $pagetable_t pte_get_addr(pte);
    u64 ind = vaddr_get_pt_ind(vaddr, 0);
    assert(!(pt[ind] & PTE_PRESENT), "pt_map_page() : entry corresponding to physical page should not be present");

    //map the page
    pt[ind] = pte_create_new(paddr, leaf_flags);
}

//creates a pagetable mapping from vaddr -> paddr
//maps hugepg (2MB)
//panics if vaddr is already mapped
//vaddr, paddr must be 
void pt_map_hugepg(pagetable_t pt, void* vaddr, void* paddr, u64 leaf_flags) {  
    //ensure that vaddr, paddr are hugepg aligned
    assert(($u64 vaddr) % HUGEPG_SIZE == 0x0, "pt_map_page() : vaddr must be page aligned");
    assert(($u64 paddr) % HUGEPG_SIZE == 0x0, "pt_map_page() : paddr must be page aligned");

    //check if some vaddr here is already mapped
    //TODO (i think this already gets done when we traverse down to level 1)

    //traverse pt to level 2 
    i32 level = 2;
    pte_t pte = pt_walk(pt, vaddr, level, 1, PTE_WRITEABLE, leaf_flags);
    assert(level == 2, "pt_map_hugepg() : we should get level 2 pte");
    assert(!(pte & PTE_HUGEPG), "pt_map_hugepg() : pte should not correspond to huge page");

    //traverse down to level 1
    pt = $pagetable_t pte_get_addr(pte);
    u64 ind = vaddr_get_pt_ind(vaddr, 1);
    assert(!(pt[ind] & PTE_PRESENT), "pt_map_hugepg() : entry corresponding to physical page should not be present");

    //map the hugepg
    pt[ind] = pte_create_new(paddr, leaf_flags | PTE_HUGEPG);
}

//overwrites the flags of this vaddr mapping to be equal to leaf_flags
//vaddr must be mapped using a regular sized page (4KB)
void pt_set_leaf_flags(pagetable_t pt, void* vaddr, u64 leaf_flags) {
    //ensure that vaddr is page aligned and mapped
    assert(($u64 vaddr) % PAGE_SIZE == 0x0, "pt_set_leaf_flags() : vaddr must be page aligned");
    assert(pt_is_vaddr_mapped(pt, vaddr), "pt_set_leaf_flags() : vaddr should be mapped");

    //traverse pt to level 1
    i32 level = 1;
    pte_t pte = pt_walk(pt, vaddr, level, 0, PTE_WRITEABLE, leaf_flags);
    assert(level == 1, "pt_set_leaf_flags() : we should get level 1 pte");
    assert(!(pte & PTE_HUGEPG), "pt_set_leaf_flags() : pte should not correspond to huge page");

    //traverse down to level 0 pt
    pt = $pagetable_t pte_get_addr(pte);
    u64 ind = vaddr_get_pt_ind(vaddr, 0);
    assert((pt[ind] & PTE_PRESENT) == PTE_PRESENT, "pt_set_leaf_flags() : entry corresponding to physical page should be present");

    //overwrite the flags
    void* paddr = pte_get_addr(pt[ind]);
    pt[ind] = pte_create_new(paddr, leaf_flags);
}

void pt_alloc_and_map_page(pagetable_t pt, void* vaddr, u64 leaf_flags) {
    pt_map_page(pt, vaddr, palloc(), leaf_flags);
}

void pt_map_page_if_not_mapped(pagetable_t pt, void* vaddr, void* paddr, u64 leaf_flags) {
    if(!pt_is_vaddr_mapped(pt, vaddr)) {
        pt_map_page(pt, vaddr, paddr, leaf_flags);
    }
}

//removes pagetable mapping corresponding to vaddr, does not free the physical page
//panics if vaddr is not mapped
void pt_unmap_page(pagetable_t pt, void* vaddr) {
    //ensure this page is mapped
    assert(pt_is_vaddr_mapped(pt, vaddr), "pt_unmap_page() : vaddr not mapped");

    //traverse pt to level 1
    i32 level = 1;
    pte_t pte = pt_walk(pt, vaddr, level);
    assert(level == 1, "pt_unmap_page() : we should get level 1 pte");
    assert(!(pte & PTE_HUGEPG), "pte_unmap_page() : pte should not correspond to huge page");

    //traverse down to level 0 pt
    pt = $pagetable_t pte_get_addr(pte);
    u64 ind = vaddr_get_pt_ind(vaddr, 0);
    assert(pt[ind] & PTE_PRESENT, "pt_unmap_page() : entry corresponding to physical page should be present");

    //unmap the page
    pt[ind] = $pte_t 0;
}

//translates vaddr into paddr
void* pt_translate(pagetable_t pt, void* vaddr, u64& out_perm_flags) {
    i32 level = 0;
    pte_t pte = pt_walk(pt, vaddr, level);
    out_perm_flags = pte_get_permission_flags(pte);
    assert(level < 3, "pt_translate() : resulting level should be one of 0, 1, 2");
    if(level == 2)      return $void* ($u64 pte_get_addr(pte) + ($u64 vaddr & 0x3fffffff)); // 1GB hugepg
    else if(level == 1) return $void* ($u64 pte_get_addr(pte) + ($u64 vaddr & 0x001fffff)); // 2MB hugepg
    else if(level == 0) return $void* ($u64 pte_get_addr(pte) + ($u64 vaddr & 0x00000fff)); // 4KB page
    panic("pt_translate() : unexpected level");
    return nullptr;
}

void* pt_translate(pagetable_t pt, void* vaddr) {
    u64 out_perm_flags;
    return pt_translate(pt, vaddr, out_perm_flags);
}

//recursively frees the entire pagetable and the underlying physical pages. 
//assumes that each physical page is only mapped once. 
void pt_free(pagetable_t pt, i32 level) {
    for(i32 i = 0; i < 512; i++){
        pte_t pte = pt[i];
        if(!(pte & PTE_PRESENT)) continue;

        //if this is not a leaf, free subtree
        if(level) pt_free($pagetable_t pte_get_addr(pte), level - 1);

        //if this is a leaf, free the mem directly
        if(!level) pfree($void* pte_get_addr(pte));
    }

    //free this pt
    pfree($void* pt);
}

void pt_free(pagetable_t pt) {
    pt_free(pt, 3);
}

//frees everything below virtual address 0x00007fffffffffff
// test this
void pt_free_lower_half(pagetable_t pt) {   
    for(i32 i = 0; i < 256; i++){
        pte_t pte = pt[i];
        if(!(pte & PTE_PRESENT)) continue;
        pt_free($pagetable_t pte_get_addr(pte), 2);
    }
}

//copies all mappings from pt to dest_pt that exist in the lower half
// so below virtual address 0x00007fffffffffff (user space)
//allocs new pages and copies the data over as well
void pt_copy_lower_half(pagetable_t pt, pagetable_t dest_pt, u64 vaddr, i32 level) {
    for(u64 i = 0x0; i < $u64 512; i++){
        if(level == 3 && i == $u64 256) break; 

        u64 cvaddr = vaddr | (i << ($u64 12 + $u64 (9 * level)));
        pte_t pte = pt[i];    
        if(!(pte & PTE_PRESENT)) continue;
        pagetable_t npt = $pagetable_t pte_get_addr(pte);

        //if this is not a leaf, copy subtree
        if(level) pt_copy_lower_half(npt, dest_pt, cvaddr, level - 1);

        //if this is a leaf, copy mapping
        if(!level) {
            u64 flags = pte_get_permission_flags(pte);
            void* paddr = $void* pte_get_addr(pte);
            void* npaddr = palloc();

            pt_map_page(dest_pt, $void* cvaddr, npaddr, flags);
            memcpy(npaddr, paddr, PAGE_SIZE);
        }
    }
}

void pt_copy_lower_half(pagetable_t pt, pagetable_t dest_pt) {
    pt_copy_lower_half(pt, dest_pt, 0x0, 3);
}

void pt_print(pagetable_t pt, i32 level, i32 indent) {
    for(i32 i = 0; i < 512; i++){
        pte_t pte = pt[i];
        if(!(pte & PTE_PRESENT)) continue;
        if(pte & PTE_HUGEPG) continue;
        if(level) pt_print($pagetable_t pte_get_addr(pte), level - 1, indent + 1);
    }
    for(i32 i = 0; i < indent; i++) print("    ");
    println(pt);
}

void pt_print(pagetable_t pt) {
    pt_print(pt, 3, 0);
}


