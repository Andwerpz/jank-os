#include "pma.jank";
#include "memlayout.jank";

//right now, we kind of have to handle huge pages as BOOTBOOT initializes our kernel pagetable with hugepages.

typedef u64 pte_t;              
typedef pte_t* pagetable_t;

[__GLOBAL_FIRST__] u64 PTE_PRESENT    = $u64 1 << $u64 0;
[__GLOBAL_FIRST__] u64 PTE_WRITEABLE  = $u64 1 << $u64 1;
[__GLOBAL_FIRST__] u64 PTE_USER       = $u64 1 << $u64 2;   //if set, user will be able to access
[__GLOBAL_FIRST__] u64 PTE_HUGEPG     = $u64 1 << $u64 7;
[__GLOBAL_FIRST__] u64 PTE_NX         = $u64 1 << $u64 63;  //*NOT* executable flag, only valid on leaf PTEs?
[__GLOBAL_FIRST__] u64 PTE_PCD        = $u64 1 << $u64 4;   // Cache Disable

void enable_nx() {
    //check if nx is supported
    u64 x;
    asm!("mov $0x80000001, %eax");
    asm!("cpuid");
    asm!("mov %edx, {x}");
    passert(x & $u64 (1 << 20), "enable_nx() : NX should be supported");

    //enable NX bit in EFER
    wrmsr(EFER_MSR, rdmsr(EFER_MSR) | $u64 (1 << 11));
}

i32 init_paging() {
    enable_nx();
    return 0;
}

void* pte_get_addr(pte_t pte) {
    return $void* (pte & 0x7fffffffff000);
}

u64 pte_get_permission_flags(pte_t pte) {
    u64 flag_mask = PTE_PRESENT | PTE_USER | PTE_WRITEABLE | PTE_NX;
    return pte & flag_mask;
}

pte_t pte_create_new(u64 flags) {
    pagetable_t pt = pt_alloc_new();
    return ($u64 pt) | flags | PTE_PRESENT | PTE_USER;
}

pte_t pte_create_new(void* paddr, u64 flags) {
    return ($u64 paddr) | flags | PTE_PRESENT | PTE_USER;
}

u64 vaddr_get_pt_ind(void* vaddr, i32 level) {
    passert(3 >= level && level >= 0, "vaddr_get_pt_ind() : invalid level");
    return (($u64 vaddr) & (0x1ff << $u64 (12 + level * 9))) >> $u64 (12 + level * 9);
}

pagetable_t pt_alloc_new() {
    pagetable_t pt = $pagetable_t palloc();
    memset($void* pt, 0, PAGE_SIZE);
    return pt;
}

pagetable_t pt_get_current() {
    pagetable_t ret;
    asm!("mov %cr3, %rax");
    asm!("mov %rax, {ret}");
    return ret;
}

void pt_switch(pagetable_t npt) {
    asm!("movq {npt}, %rax");
    asm!("mov %rax, %cr3");
}

void pt_switch_to_kernel() {
    asm!("movq %gs:0, %rax");
    asm!("mov %rax, %cr3");
}

//given virtual page address, walks along the provided pagetable
//stops traversing at level 'level' and returns the PTE to the next page at that level
// if(create), then it creates stuff wherever needed
// else, it returns the PTE where it stopped (so PTE_PRESENT should be unset)
//updates level to indicate the actual level reached
pte_t pt_walk(pagetable_t pt, void* vaddr, i32& level, i32 create, u64 flags, u64 leaf_flags) {
    passert($u64 vaddr % PAGE_SIZE == $u64 0, "pt_walk() : vaddr must be page aligned");
    passert(3 >= level && level >= 0, "pt_walk() : invalid level");
    //serial_print("PT WALK VADDR : ");
    //serial_println(vaddr);
    i32 _level = level;
    level = 3;
    for(; level >= _level; level--){
        u64 ind = vaddr_get_pt_ind(vaddr, level);
        pte_t pte = pt[ind];

        //check if pte is not present
        if(!(pte & PTE_PRESENT)) {
            //if create flag is not set, return non-present pte
            if(!create) return pte;

            //create new page and map it
            if(level != 0) pt[ind] = pte_create_new(flags);
            else pt[ind] = pte_create_new(leaf_flags);
            pte = pt[ind];
        }

        //check if this is a huge page
        if(level == 2 && $i32 (pte & PTE_HUGEPG)) {
            return pte;
        }
        if(level == 1 && $i32 (pte & PTE_HUGEPG)) {
            return pte;
        }

        //check if we should stop
        if(level == _level) {
            return pte;
        }

        //traverse down
        pt = $pagetable_t pte_get_addr(pte);
    }

    panic("pt_walk() : should not exit for loop");
    return $pte_t 0;
}

//nocreate pt_walk
//panics if vaddr is not mapped to the specified level
pte_t pt_walk(pagetable_t pt, void* vaddr, i32& level) {
    pte_t pte = pt_walk(pt, vaddr, level, 0, $u64 0, $u64 0);
    passert(pte & PTE_PRESENT, "pt_walk() : vaddr mapping not found");
    return pte;
}

i32 pt_is_vaddr_mapped(pagetable_t pt, void* vaddr) {
    i32 level = 0;
    pte_t pte = pt_walk(pt, vaddr, level, 0, $u64 0, $u64 0);
    return $i32 (pte & PTE_PRESENT);
}

//creates a pagetable mapping from vaddr -> paddr
//panics if vaddr is already mapped
void pt_map_page(pagetable_t pt, void* vaddr, void* paddr, u64 leaf_flags) {
    //check if this page is already mapped
    passert(!pt_is_vaddr_mapped(pt, vaddr), "pt_map_page() : vaddr already mapped");

    //traverse pt to level 1
    i32 level = 1;
    pte_t pte = pt_walk(pt, vaddr, level, 1, PTE_WRITEABLE, leaf_flags);
    passert(level == 1, "pt_map_page() : we should get level 1 pte");
    passert(!(pte & PTE_HUGEPG), "pt_map_page() : pte should not correspond to huge page");

    //traverse down to level 0 pt
    pt = $pagetable_t pte_get_addr(pte);
    u64 ind = vaddr_get_pt_ind(vaddr, 0);
    passert(!(pt[ind] & PTE_PRESENT), "pt_map_page() : entry corresponding to physical page should not be present");

    //map the page
    pt[ind] = pte_create_new(paddr, leaf_flags);
}

void pt_alloc_and_map_page(pagetable_t pt, void* vaddr, u64 leaf_flags) {
    pt_map_page(pt, vaddr, palloc(), leaf_flags);
}

//removes pagetable mapping corresponding to vaddr, does not free the physical page
//panics if vaddr is not mapped
void pt_unmap_page(pagetable_t pt, void* vaddr) {
    //ensure this page is mapped
    passert(pt_is_vaddr_mapped(pt, vaddr), "pt_unmap_page() : vaddr not mapped");

    //traverse pt to level 1
    i32 level = 1;
    pte_t pte = pt_walk(pt, vaddr, level);
    passert(level == 1, "pt_unmap_page() : we should get level 1 pte");
    passert(!(pte & PTE_HUGEPG), "pte_unmap_page() : pte should not correspond to huge page");

    //traverse down to level 0 pt
    pt = $pagetable_t pte_get_addr(pte);
    u64 ind = vaddr_get_pt_ind(vaddr, 0);
    passert(pt[ind] & PTE_PRESENT, "pt_unmap_page() : entry corresponding to physical page should be present");

    //unmap the page
    pt[ind] = $pte_t 0;
}

//translates vaddr into paddr
void* pt_translate(pagetable_t pt, void* vaddr, u64& out_perm_flags) {
    i32 level = 0;
    pte_t pte = pt_walk(pt, vaddr, level);
    out_perm_flags = pte_get_permission_flags(pte);
    passert(level < 3, "pt_translate() : resulting level should be one of 0, 1, 2");
    if(level == 2)      return $void* (($u64 pte_get_addr(pte) << $u64 18) + ($u64 vaddr & 0x3fffffff));
    else if(level == 1) return $void* (($u64 pte_get_addr(pte) << $u64  9) + ($u64 vaddr & 0x001fffff));
    else if(level == 0) return $void* (($u64 pte_get_addr(pte) << $u64  0) + ($u64 vaddr & 0x00000fff));
    panic("pt_translate() : unexpected level");
    return nullptr;
}

void* pt_translate(pagetable_t pt, void* vaddr) {
    u64 out_perm_flags;
    return pt_translate(pt, vaddr, out_perm_flags);
}

//recursively frees the entire pagetable and the underlying physical pages. 
//assumes that each physical page is only mapped once. 
void pt_free(pagetable_t pt, i32 level) {
    for(i32 i = 0; i < 512; i++){
        pte_t pte = pt[i];
        if(!(pte & PTE_PRESENT)) continue;

        //if this is not a leaf, free subtree
        if(level) pt_free($pagetable_t pte_get_addr(pte), level - 1);

        //if this is a leaf, free the mem directly
        if(!level) pfree($void* pte_get_addr(pte));
    }

    //free this pt
    pfree($void* pt);
}

void pt_free(pagetable_t pt) {
    pt_free(pt, 3);
}

//frees everything below virtual address 0x00007fffffffffff
// test this
void pt_free_lower_half(pagetable_t pt) {   
    for(i32 i = 0; i < 256; i++){
        pte_t pte = pt[i];
        if(!(pte & PTE_PRESENT)) continue;
        pt_free($pagetable_t pte_get_addr(pte), 2);
    }
}

//copies all mappings from pt to dest_pt that exist in the lower half
// so below virtual address 0x00007fffffffffff (user space)
//allocs new pages and copies the data over as well
void pt_copy_lower_half(pagetable_t pt, pagetable_t dest_pt, u64 vaddr, i32 level) {
    for(u64 i = 0x0; i < $u64 512; i++){
        if(level == 3 && i == $u64 256) break; 

        u64 cvaddr = vaddr | (i << ($u64 12 + $u64 (9 * level)));
        pte_t pte = pt[i];    
        if(!(pte & PTE_PRESENT)) continue;
        pagetable_t npt = $pagetable_t pte_get_addr(pte);

        //if this is not a leaf, copy subtree
        if(level) pt_copy_lower_half(npt, dest_pt, cvaddr, level - 1);

        //if this is a leaf, copy mapping
        if(!level) {
            u64 flags = pte_get_permission_flags(pte);
            void* paddr = $void* pte_get_addr(pte);
            void* npaddr = palloc();

            pt_map_page(dest_pt, $void* cvaddr, npaddr, flags);
            memcpy(npaddr, paddr, PAGE_SIZE);
        }
    }
}

void pt_copy_lower_half(pagetable_t pt, pagetable_t dest_pt) {
    pt_copy_lower_half(pt, dest_pt, 0x0, 3);
}

void pt_print(pagetable_t pt, i32 level, i32 indent) {
    for(i32 i = 0; i < 512; i++){
        pte_t pte = pt[i];
        if(!(pte & PTE_PRESENT)) continue;
        if(pte & PTE_HUGEPG) continue;
        if(level) pt_print($pagetable_t pte_get_addr(pte), level - 1, indent + 1);
    }
    for(i32 i = 0; i < indent; i++) serial_print("    ");
    serial_println(pt);
}


